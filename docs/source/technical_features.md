# Technical features

* [Physic informed network](#physic-informed-network)
* [Uncertainty measures](#uncertainty-measures)
* [Two step training scheme with class sampling](#two-steps-training-scheme-with-class-sampling)
* [MixUp](#mixup)
* [Data augmentation on lesion labels](#data-augmentation-on-lesion-labels)
* [Network architectures](#network-architectures)
* [Loss functions](#loss-functions)

## Physic informed network
CNNs can be modulated, at each layer, using the [Feature-wise Linear Modulation (FiLM)](https://arxiv.org/pdf/1709.07871.pdf) technique. FiLM permits to add priors during training/inference
based on the imaging physics (e.g. acquisition parameters), thereby improving the
performance of the output segmentations.

![Figure FiLM](https://github.com/neuropoly/ivado-medical-imaging/raw/master/images/film_figure.png)

## Uncertainty measures
At inference time, uncertainty can be estimated via two ways:
- model-based uncertainty (epistemic) based on [Monte Carlo Dropout](https://arxiv.org/pdf/1506.02142.pdf).
- image-based uncertainty (aleatoric) [based on test-time augmentation](https://doi.org/10.1016/j.neucom.2019.01.103).

From the Monte Carlo samples, different measures of uncertainty can be derived:
- voxel-wise entropy
- structure-wise intersection over union
- structure-wise coefficient of variation
- structure-wise averaged voxel-wise uncertainty within the structure

These measures can be used to perform some [post-processing](https://arxiv.org/pdf/1808.01200.pdf) based on the uncertainty measures.

![Figure Uncertainty](https://github.com/neuropoly/ivado-medical-imaging/raw/master/images/uncertainty_measures.png)

## Two steps training scheme with class sampling
Class sampling, coupled with a transfer learning strategy, can mitigate class
imbalance issues, while addressing the limitations of classical under-sampling
(risk of loss of information) or over-sampling (risk of overfitting) approaches.

During a first training step, the CNN is trained on an equivalent proportion of
positive and negative samples, negative samples being under-weighted dynamically
at each epoch. During the second step, the CNN is fine-tuned on the realistic
(i.e. class-imbalanced) dataset.

## Mixup
[Mixup](https://arxiv.org/abs/1710.09412) is a data augmentation technique,
wherein training is performed on samples that are generated by combining two
random samples from the training set and from the associated labels. The motivation
is to regularize the network while extending the training distribution.

![Figure mixup](https://github.com/neuropoly/ivado-medical-imaging/raw/master/images/mixup.png)

## Data augmentation on lesion labels
This data augmentation is motivated by the large inter-rater variability that is
typical in medical image segmentation tasks. Typically, raters disagree on the boundaries
of pathologies (e.g., tumors, lesions). A soft mask is constructed by morphological
dilation of the binary segmentation (i.e. mask provided by expert), where
expert-labeled voxels have one as value while the augmented voxels are assigned
a soft value which depends on the distance to the core of the lesion. Thus,
the prior knowledge about the subjective lesion borders is then leveraged to the network.

![Figure Data Augmentation on lesion ground truths](https://github.com/neuropoly/ivado-medical-imaging/raw/master/images/dilate-gt.png)

## Network architectures
- [U-net](https://arxiv.org/pdf/1505.04597.pdf)
- [HeMIS](https://arxiv.org/abs/1607.05194)

## Loss functions
- [Dice Loss](https://arxiv.org/abs/1606.04797)
- [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf)
- [Generalised Dice Loss](https://arxiv.org/pdf/1707.03237.pdf)
